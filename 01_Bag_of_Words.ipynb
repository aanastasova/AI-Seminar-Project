{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxW8k7D8l-7"
      },
      "source": [
        "# Introduction to Bag of Words\n",
        "* Text Retrieval and Mining, BSc BAN, 2023-2024\n",
        "* Author: [Julien Rossi](mailto:j.rossi@uva.nl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7os8wyd8pyG"
      },
      "source": [
        "Bag-of-Words is a family of text representations, where text vectors are built by observing and counting the words that appear in a text.\n",
        "\n",
        "We study 2 types of BoW vectors:\n",
        "* **Raw Count**: actually count the number of occurences of each word in a text\n",
        "* **TF-IDF**: adjust the raw count to favor words that appear a lot in a few documents, as opposed to those who appear a lot in all documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkNx4UuJSCqr"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxmX4zvSJSN"
      },
      "source": [
        "**Document** and **Corpus**:\n",
        "* **Document** is the smallest unit of text of your use case\n",
        "* **Corpus** is your collection of documents\n",
        "* **Use case**: think of the typical question you are looking the answer to\n",
        "* **Query**: the text you will use to search in your corpus\n",
        "\n",
        "A few examples of use cases:\n",
        "* Use case 1: \"*which academic papers are about black holes?*\"\n",
        "   * Corpus: academic papers uploaded to ArXiv\n",
        "   * Document: 1 paper\n",
        "   * Query: \"black hole\"\n",
        "* Use case 2: \"*Where does Victor Hugo mention Notre-Dame?*\"\n",
        "   * Corpus: entire works from Victor Hugo\n",
        "   * Document: 1 paragraph\n",
        "   * Query: \"notre dame\"\n",
        "* Use case 3: \"*What can I cook with pasta and garlic?*\"\n",
        "   * Corpus: all recipes in multiple cook books\n",
        "   * Document: 1 recipe\n",
        "   * Query: \"pasta garlic\"\n",
        "\n",
        "**Tokenizer**\n",
        "\n",
        "A tokenizer is a program that takes in a text and splits it into smaller units. A book can be split into chapters, into paragraphs, into sentences, into words. Those are all examples of tokenization process.\n",
        "\n",
        "Once a text is tokenized into sentences, you can tokenize sentences into words.\n",
        "\n",
        "\n",
        "**Sentence**\n",
        "\n",
        "In natural language, a text is made of multiple sentences, separated by punctuation marks such as `.`. It is nonetheless a challenge to split a text into sentences as some `.` indicate abbreviations, for example.\n",
        "\n",
        "**Word**:\n",
        "\n",
        "Any text is made of words. Sometimes they are nicely separated by spaces or punctuation marks. As with sentences, some words include punctuation marks, like `U.S.A.`, or `to court-martial`.\n",
        "\n",
        "\n",
        "**Vocabulary**:\n",
        "\n",
        "The list of unique words used in the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiHIb5UdYmjb"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utaw9nXe9UfO"
      },
      "source": [
        "## Download Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4HBqOrd9Ww7"
      },
      "source": [
        "We will use some short extracts from a Sherlock Holmes story \"Scandal in Bohemia\", by Sir Arthur Conan Doyle.\n",
        "\n",
        "We will start with the first paragraph of the book.\n",
        "\n",
        "* **Corpus**: All sentences in \"Scandal in Bohemia\"\n",
        "* **Document**: 1 sentence of the book"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgFygmknSgGz"
      },
      "source": [
        "import requests\n",
        "\n",
        "r = requests.get('https://sherlock-holm.es/stories/plain-text/scan.txt')\n",
        "\n",
        "assert r.status_code == 200\n",
        "\n",
        "with open('scandal_in_bohemia.txt', 'w') as out:\n",
        "    out.write(r.content.decode('utf-8'))\n",
        "lines = [txt for txt in open('scandal_in_bohemia.txt') if len(txt.strip()) > 0]\n",
        "\n",
        "print(lines[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4uzaTvj3DcO"
      },
      "source": [
        "# First Paragraph\n",
        "par = ' '.join([x.strip() for x in lines[7:25]])\n",
        "\n",
        "import textwrap\n",
        "print(textwrap.fill(par, width=80))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6aj-8EP9zk1"
      },
      "source": [
        "## NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy-4tp9b90_z"
      },
      "source": [
        "NLTK is a Python library for text analytics.\n",
        "\n",
        "See [Link](https://www.nltk.org)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ucJUYSk3y4F"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z2AHyMD-Cg1"
      },
      "source": [
        "The **sentence tokenizer** takes care to split a text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHQTYmh1IiJ"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk_sentences = sent_tokenize(par)\n",
        "nltk_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sehd2Q1-JLG"
      },
      "source": [
        "The **word tokenizer** takes care to split a text into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fafyGLil4Qnf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk_tokens = word_tokenize(nltk_sentences[0])\n",
        "nltk_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytFuVivZ-PDq"
      },
      "source": [
        "## SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty345RqK-Qjx"
      },
      "source": [
        "SpaCy is another Python libary for text analytics.\n",
        "\n",
        "See [Link](https://spacy.io)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lsRw3uu1JL9"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_b6HUoO1l5U"
      },
      "source": [
        "doc = nlp(par)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n1Np6A6-aXz"
      },
      "source": [
        "It has also a **sentence tokenizer**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2k0L6Rm2X7d"
      },
      "source": [
        "spacy_sentences = list(doc.sents)\n",
        "spacy_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhm2pX8-e8L"
      },
      "source": [
        "And a **word tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvJDp4Vs4Lr7"
      },
      "source": [
        "spacy_tokens = [x for x in spacy_sentences[0]]\n",
        "spacy_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x64r_ta5-hMk"
      },
      "source": [
        "**Warning**: NLTK / SpaCy might produce different results: break sentences at different places, break words at different places, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6DVZqO4Q8Oj"
      },
      "source": [
        "s = nltk_sentences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRCYWOP1RS5K"
      },
      "source": [
        "## SKLEARN Generalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE4wvrdYRWnX"
      },
      "source": [
        "Classes likes `CountVectorizer` or `TfidfVectorizer` works in the following way:\n",
        "* Instantiate an object with specific parameters (`v = CountVectorizer(...)`)\n",
        "* Fit this object to your corpus = learn the vocabulary (method `v.fit(...)`)\n",
        "* Transform any piece of text you have into a vector (method `v.transform()`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNuZNnD-elyo"
      },
      "source": [
        "def show_vocabulary(vectorizer):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f'Vocabulary size: {len(words)} words')\n",
        "\n",
        "    # we can print ~10 words per line\n",
        "    for l in np.array_split(words, math.ceil(len(words) / 10)):\n",
        "        print(''.join([f'{x:<15}' for x in l]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxgmPDksgFP-"
      },
      "source": [
        "import os\n",
        "os.environ[\"FORCE_COLOR\"] = \"1\"\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "def show_bow(vectorizer, bow):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # we can print ~8 words + coefs per line\n",
        "    for l in np.array_split(list(zip(words, bow)), math.ceil(len(words) / 8)):\n",
        "        print(' | '.join([colored(f'{w:<15}:{n:>2}', 'grey') if int(n) == 0 else colored(f'{w:<15}:{n:>2}', on_color='on_yellow', attrs=['bold']) for w, n in l ]))\n",
        "\n",
        "def show_bow_float(vectorizer, bow):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # we can print ~6 words + coefs per line\n",
        "    for l in np.array_split(list(zip(words, bow)), math.ceil(len(words) / 6)):\n",
        "        print(' | '.join([colored(f'{w:<15}:{float(n):>0.2f}', 'grey') if float(n) == 0 else colored(f'{w:<15}:{float(n):>0.2f}', on_color='on_yellow', attrs=['bold']) for w, n in l ]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19FRjfBh-qXU"
      },
      "source": [
        "# Raw Count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzl-bvmhXI_G"
      },
      "source": [
        "* We take a text, any text, and represent it as a vector\n",
        "* Each text is represented by a vector with **N** dimensions\n",
        "* Each dimension is representative of **1 word** of the vocabulary\n",
        "* The coefficient in dimension **k** is the number of times the word at index **k** in the vocabulary is seen in the represented text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBDkKOmw4ZVr"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaZa3oL1ZM5u"
      },
      "source": [
        "## First Example - Reduced Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6pZWy_cZSQH"
      },
      "source": [
        "We illustrate with a small corpus so we have a reduced vocabulary.\n",
        "\n",
        "* **Corpus**: The first paragraph of the book\n",
        "* **Document**: 1 sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0hlL80hZMAQ"
      },
      "source": [
        "count_small = CountVectorizer(lowercase=False)\n",
        "count_small.fit(nltk_sentences)\n",
        "show_vocabulary(count_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVt-wZMFfRjQ"
      },
      "source": [
        "The option `lowercase` sets up one behavior of the raw count: do we consider `And` to be different than `and`?\n",
        "\n",
        "* `lowercase=False` gives 134 unique words in the vocabulary\n",
        "* `lowercase=True` gives 127 unique words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgtMIwC_ebmo"
      },
      "source": [
        "count_small = CountVectorizer(lowercase=True)\n",
        "count_small.fit(nltk_sentences)\n",
        "show_vocabulary(count_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF1tqrBN6Rek"
      },
      "source": [
        "s = nltk_sentences[0]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow = count_small.transform([s])\n",
        "print(f'BoW Shape: {bow.shape}')\n",
        "bow = bow.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGNO80UY6tCY"
      },
      "source": [
        "show_bow(count_small, bow[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goqoQQ-Tc06Z"
      },
      "source": [
        "## Second Example - Larger Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3xgDbewc8By"
      },
      "source": [
        "* **Corpus**: entire book\n",
        "* **Document**: 1 sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_7vdg2odSdZ"
      },
      "source": [
        "book = ' '.join([x.strip() for x in lines])\n",
        "sentences = sent_tokenize(book)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2pvsks18L_D"
      },
      "source": [
        "count = CountVectorizer(lowercase=True)\n",
        "count.fit(sentences)\n",
        "show_vocabulary(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2dUIgjd2fy"
      },
      "source": [
        "s = sentences[10]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow = count.transform([s])\n",
        "print(f'BoW Shape: {bow.shape}')\n",
        "bow = bow.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn3qBOc0lz1Q"
      },
      "source": [
        "show_bow(count, bow[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHyfUpJJmdbl"
      },
      "source": [
        "## Real-Life Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1a6hshWoC_H"
      },
      "source": [
        "Books are very clean texts. Real-Life corpuses including user-generated material will be on the opposite of the spectrum, and will include typos, strange usernames, artefacts of all kinds...\n",
        "\n",
        "The \"20 newsgroups\" dataset is a classical NLP dataset. Newsgroups are the ancestors of reddit, people could post messages and reply in a thread.\n",
        "\n",
        "* **Corpus**: newsgroup messages\n",
        "* **Document**: full text of 1 message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV7JF84EmmLO"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5a-35WInID6"
      },
      "source": [
        "newsgroups = fetch_20newsgroups()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Me_M00nQC6"
      },
      "source": [
        "print(f'Number of documents: {len(newsgroups.data)}')\n",
        "print(f'Sample document:\\n{newsgroups.data[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMeO45zLpJQm"
      },
      "source": [
        "* Vocabulary is much larger (130107 unique words)\n",
        "* Lots of \"garbage\" in vocabulary (\"mbocjlo3\", \"mc2i\", \"mc68882rc25\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrBhPPaaneBW"
      },
      "source": [
        "count = CountVectorizer()\n",
        "count.fit(newsgroups.data)\n",
        "show_vocabulary(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIViZ5Jpo70f"
      },
      "source": [
        "print(f'Size of vocabulary: {len(count.get_feature_names_out())}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs3QZ_6zqUq1"
      },
      "source": [
        "# Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFCwrFKcqX6X"
      },
      "source": [
        "With these vectors, we can build a search engine.\n",
        "\n",
        "* **Query**: Let the user enter a text query\n",
        "* Search through the corpus the documents that are **similar** to the query\n",
        "* **Similarity**: we use the **cosine similary** of the BoW vectors of two texts to evaluate their similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **CORPUS**: postings from 20 newsgroups\n",
        "* **QUERY**: keywords entered by user\n",
        "* **SCORING**: cosine similarity between BoW representations\n",
        "* **EVALUATION**: unfortunately none, we need some qrels"
      ],
      "metadata": {
        "id": "jFKybs3InY6M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF21fhqssnNP"
      },
      "source": [
        "corpus_bow = count.transform(newsgroups.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvy6QV7esHvK"
      },
      "source": [
        "query = input(\"Type your query: \")\n",
        "query_bow = count.transform([query])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtszmsEhsQWk"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(corpus_bow, query_bow)\n",
        "print(f'Similarity Matrix Shape: {similarity_matrix.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "669vjo2Yu8qq"
      },
      "source": [
        "The similarity matrix has **D** rows (the number of documents in the corpus) and 1 column.\n",
        "\n",
        "Coefficient at row **k** is the cosine similarity between the document at index **k** in the corpus and the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5HRmO4Ks7D1"
      },
      "source": [
        "similarities = pd.Series(similarity_matrix[:, 0])\n",
        "similarities.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYIShCK9tQX9"
      },
      "source": [
        "top_10 = similarities.sort_values(ascending=False)[:10]\n",
        "top_10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkGTSYvGtgHT"
      },
      "source": [
        "print('Best document:')\n",
        "print(newsgroups.data[top_10.index[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-ri9kfDxmuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cW6njq6qK_y"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfqMSdnUqPai"
      },
      "source": [
        "The basic for TF-IDF is that cosine similarity with raw count coefficients puts too much emphasis on the number of occurences of a word within a document.\n",
        "\n",
        "Repeating a word will artifically increase the cosine similarity with any text containing this word.\n",
        "\n",
        "Consider which word would be important:\n",
        "1. One that is repeated a lot and equally present in each document\n",
        "1. One that appears a lot only in a few document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEB4nXBawmd2"
      },
      "source": [
        "TF-IDF computes coefficients:\n",
        "* Low values for common words (ie present in the document, but quite common over the corpus)\n",
        "* High values for uncommon words (ie present in the document, but not common over the corpus)\n",
        "\n",
        "We consider one specific document, and one specific word.\n",
        "\n",
        "* **TF = Term Frequency**: the number of times the word appears in the document\n",
        "* **DF = Document Frequency**: the number of document in the corpus, in which the word appears\n",
        "* **IDF = Inverse Document Frequency**: the inverse of the Document Frequency.\n",
        "\n",
        "Logarithms are introduced, to reflect that 100 times a word does not deliver 100 times the information.\n",
        "\n",
        "Given a word **w**, a document **d** in a corpus of **D** documents:\n",
        "\n",
        "$\\textrm{TF-IDF(w, d) = TF(w, d) * IDF(w)}$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\textrm{IDF(w) = log} \\left( \\frac{1 + \\textrm{D}}{1 + \\textrm{DF(w)}} \\right) + 1\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "This is the default SKLEARN formula (see [Link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYMJJgAE0u-f"
      },
      "source": [
        "Bag of Words vectors with TF-IDF coefficients (often called TF-IDF vectors):\n",
        "* **N** dimensions, where **N** is the size of the vocabulary\n",
        "* Coefficient at dimension **k** is the coefficient for the word at index **k** in the vocabulary\n",
        "* Coefficients are TF-IDF coefficients, instead of raw count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCQ_8MS1qNz1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IIVcdsO6qra"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7qqWhr6wTT"
      },
      "source": [
        "We continue with the Sherlock Holmes book \"Scandal in Bohemia\"\n",
        "\n",
        "* **Corpus**: full text of the book\n",
        "* **Document**: 1 sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6PM-Bh1q-6"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(sentences)\n",
        "show_vocabulary(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5dQoJk10qM"
      },
      "source": [
        "s = sentences[10]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow = tfidf.transform([s])\n",
        "print(f'BoW Shape: {bow.shape}')\n",
        "bow = bow.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxVC0BwN2Gog"
      },
      "source": [
        "show_bow_float(tfidf, bow[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRqIeHO4NwB"
      },
      "source": [
        "Display the IDF of some words.\n",
        "\n",
        "* High IDF = word that appears in few documents\n",
        "* Low IDF = word that appears in most of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpumkX102SNG"
      },
      "source": [
        "words = tfidf.get_feature_names_out()\n",
        "word = input('Word: ').lower()\n",
        "\n",
        "if word in words:\n",
        "    k = words.index(word)\n",
        "    print(f'IDF({words[k]}) = {tfidf.idf_[k]}')\n",
        "else:\n",
        "    print('Not in vocabulary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3of5oQz8RGpP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G0ZIeJY6jpc"
      },
      "source": [
        "#### More than one TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX3gG-0M5U8t"
      },
      "source": [
        "There is a family of TF-IDF formulas.\n",
        "\n",
        "Another example is the **sublinear TF**, which is then:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\textrm{TF(w, d) = 1 + log} \\left( raw count \\right)\n",
        "\\end{align}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jox4xcbG3Rwl"
      },
      "source": [
        "tfidf_sublinear = TfidfVectorizer(sublinear_tf=True)\n",
        "tfidf_sublinear.fit(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QmgzAHf500o"
      },
      "source": [
        "s = sentences[10]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow_sl = tfidf_sublinear.transform([s])\n",
        "print(f'BoW Shape: {bow_sl.shape}')\n",
        "bow_sl = bow_sl.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow_sl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C--5o_657PO"
      },
      "source": [
        "show_bow_float(tfidf_sublinear, bow_sl[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aOpWVyk6FlO"
      },
      "source": [
        "word = 'yet'\n",
        "\n",
        "index = words.tolist().index(word)\n",
        "\n",
        "bow = tfidf.transform([s]).toarray()\n",
        "\n",
        "print(f'Word: \"{word}\"')\n",
        "print(f'TF-IDF with Natural TF   = {bow[0][index]:0.4f}')\n",
        "print(f'TF-IDF with Sublinear TF = {bow_sl[0][index]:0.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVjdRZZPDkAm"
      },
      "source": [
        "Repeating a word in a text will modify the TF-IDF coefficient for this word in the text representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3alWzlf73tL"
      },
      "source": [
        "word = 'yet'\n",
        "s = sentences[10]\n",
        "s = s + ' '.join(100 * [word])\n",
        "\n",
        "bow = tfidf.transform([s]).toarray()\n",
        "bow_sl = tfidf_sublinear.transform([s]).toarray()\n",
        "\n",
        "index = words.tolist().index(word)\n",
        "print(f'Word: \"{word}\"')\n",
        "print(f'TF-IDF with Natural TF   = {bow[0][index]:0.4f}')\n",
        "print(f'TF-IDF with Sublinear TF = {bow_sl[0][index]:0.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_rKJfAqGmMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}