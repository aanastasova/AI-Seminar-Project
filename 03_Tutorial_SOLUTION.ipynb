{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TUTORIAL 2 - Corpus Analysis with LDA\n",
        "* Text Retrieval and Mining, MSc Minor DS&AI, 2023-2024\n",
        "* Author: [Julien Rossi](mailto:j.rossi@uva.nl)"
      ],
      "metadata": {
        "id": "4YU4GEIeAJFo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc3aabtAAqPb"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUk_UdPrbPES"
      },
      "source": [
        "We will use the News Articles dataset.\n",
        "\n",
        "See [Link](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR).\n",
        "\n",
        "This dataset was made for studying political bias in articles, and is made of articles from different sources, reporting on political topics.\n",
        "\n",
        "With SVD we can identify topics, and use the"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxecEpNhbPET"
      },
      "source": [
        "!wget https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/GMFCTR/IZQODZ -O NewsArticles.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kby03mR34VM"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-49n5kC4Cze"
      },
      "source": [
        "df = pd.read_csv('NewsArticles.csv', encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhWp0TsV4F7O"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gDsHF2C4m1Q"
      },
      "source": [
        "df = df[['article_id', 'title', 'text']].copy().dropna().reset_index(drop=True)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uERavaqo5B_F"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CWdQdLNjF1"
      },
      "source": [
        "df['nb_words'] = df['text'].apply(lambda x: len(x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJKt6_nxNlLk"
      },
      "source": [
        "\n",
        "_ = df['nb_words'].hist(bins=100, figsize=(9, 9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STK-Zv6oAu2z"
      },
      "source": [
        "# Clustering with Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWTbBM2RbjgP"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(\n",
        "    stop_words='english',\n",
        "    min_df=2,\n",
        "    max_df=0.8,\n",
        "    max_features=50000,\n",
        "    token_pattern=r'[a-z]{2,}',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOZ-2mBLbPEV"
      },
      "source": [
        "corpus = df['text']\n",
        "term_doc = count.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9ZAsERw9huB"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class KMedians(KMeans):\n",
        "    def _e_step(self, X):\n",
        "        self.labels_ = cosine_distances(X, self.cluster_centers_).argmin(axis=1)\n",
        "    def _average(self, X):\n",
        "        return np.median(X, axis=0)\n",
        "\n",
        "normalizer = Normalizer()\n",
        "bow_norm = normalizer.fit_transform(term_doc)\n",
        "\n",
        "\n",
        "km = KMedians(n_clusters=8, init='k-means++', max_iter=100, n_init=10)\n",
        "km.fit(bow_norm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfvVLSi9h91"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(X=km.cluster_centers_, Y=bow_norm, metric='cosine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8eUma9b9iTC"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "c = closest[3]\n",
        "d = pairwise_distances(X=bow_norm[c, :], Y=bow_norm, metric='cosine')[0]\n",
        "top_10_idx = np.argsort(d)[1:11]   # the closest to a point is itself, so we remove the TOP 1\n",
        "\n",
        "print(df.iloc[c]['title'])\n",
        "print('*' * 80)\n",
        "for i, idx in enumerate(top_10_idx):\n",
        "    print(f'#{i+1:>2} (idx={idx:4}, d={d[idx]:.2f}): {df.iloc[idx][\"title\"]}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QECkBJftA-Vp"
      },
      "source": [
        "# Clustering with LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFtq647wKOpD"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "add_stops = ['mr', 'said']\n",
        "stopped_tokenized = list(map(\n",
        "    lambda tokens: [t.text for t in tokens if len(t.text) > 1 and not t.is_stop and t.text not in add_stops],\n",
        "    nlp.tokenizer.pipe(df['text'].str.lower())\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM19mW_hMrgv"
      },
      "source": [
        "import warnings\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "dictionary = Dictionary(stopped_tokenized)\n",
        "\n",
        "# Filter out words that occur less than 5 documents, or more than 90% of the documents.\n",
        "# Same effect as min_df, max_df in CountVectorizer\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.9)\n",
        "dictionary.compactify()\n",
        "\n",
        "corpus = [dictionary.doc2bow(txt) for txt in stopped_tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYePDAsfM3Zm"
      },
      "source": [
        "K = 8\n",
        "\n",
        "lda = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=K,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=1,\n",
        "    passes=5,\n",
        "    eval_every=None,\n",
        "    random_state=42,\n",
        "    per_word_topics=True,\n",
        "    minimum_probability=0.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAw8LAkONJ8K"
      },
      "source": [
        "import math\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "nb_columns = 4\n",
        "nb_rows = math.ceil(K / nb_columns)\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "cols = cols * math.ceil(K / len(cols))\n",
        "\n",
        "cloud = WordCloud(background_color='white',\n",
        "                  width=400,\n",
        "                  height=400,\n",
        "                  max_words=10,\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda.show_topics(num_topics=K, num_words=10, formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=nb_columns, nrows=nb_rows,\n",
        "                         figsize=(4*nb_columns, 4*nb_rows),\n",
        "                         sharex=True, sharey=True)\n",
        "\n",
        "for i, (topic, ax) in enumerate(zip(topics, axes.flatten())):\n",
        "    topic_words = dict(topic[1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    ax.imshow(cloud)\n",
        "    ax.set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcVtRdFNZIw"
      },
      "source": [
        "lda_vecs = lda[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGw7Z3khcio9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lda_vecs = lda[corpus]\n",
        "\n",
        "doc_topics = np.zeros((len(corpus), K))\n",
        "for i in range(len(corpus)):\n",
        "    topics = lda_vecs[i][0]\n",
        "    for (j, v) in topics:\n",
        "        doc_topics[i][j] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vJBw6DVcPOI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class KMedians(KMeans):\n",
        "    def _e_step(self, X):\n",
        "        self.labels_ = cosine_distances(X, self.cluster_centers_).argmin(axis=1)\n",
        "    def _average(self, X):\n",
        "        return np.median(X, axis=0)\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lda_norm = normalizer.fit_transform(doc_topics)\n",
        "\n",
        "\n",
        "lda_km = KMedians(n_clusters=8, init='k-means++', max_iter=100, n_init=10)\n",
        "lda_km.fit(lda_norm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "642vq_ALcPON"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(X=lda_km.cluster_centers_, Y=lda_norm, metric='cosine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDmCOp-cPOO"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "c = closest[2]\n",
        "d = pairwise_distances(X=[lda_norm[c]], Y=lda_norm, metric='cosine')[0]\n",
        "top_10_idx = np.argsort(d)[1:11]   # the closest to a point is itself, so we remove the TOP 1\n",
        "\n",
        "print(df.iloc[c]['title'])\n",
        "print('*' * 80)\n",
        "for i, idx in enumerate(top_10_idx):\n",
        "    print(f'#{i+1:>2} (idx={idx:4}, d={d[idx]:.2f}): {df.iloc[idx][\"title\"]}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOiHsoAm8wgy"
      },
      "source": [
        "# GridSearch for Coherence Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUBfFidZbgGv"
      },
      "source": [
        "MULTICORE = False   # Switch True to use on your own multicore laptop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS3qM_IW2jPc"
      },
      "source": [
        "import tqdm\n",
        "import logging\n",
        "\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models import LdaMulticore\n",
        "\n",
        "gensim_logger = logging.getLogger('gensim.models.ldamodel')\n",
        "gensim_logger.setLevel(logging.DEBUG)\n",
        "\n",
        "history = []\n",
        "Ks = [4, 6, 8, 10, 20, 50, 100]\n",
        "\n",
        "for K in tqdm.tqdm(Ks):\n",
        "    if MULTICORE:\n",
        "        import psutil\n",
        "        NUM_CORES = psutil.cpu_count(logical=False)\n",
        "\n",
        "        lda_k = LdaMulticore(\n",
        "            corpus=corpus,\n",
        "            id2word=dictionary,\n",
        "            num_topics=K,\n",
        "            iterations=100,\n",
        "            passes=20,\n",
        "            eval_every=None,\n",
        "            random_state=42,\n",
        "            workers=NUM_CORES - 1              # Adjust it to your computer: Number of CPU Cores - 1\n",
        "        )\n",
        "    else:\n",
        "        lda_k = LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=dictionary,\n",
        "            num_topics=K,\n",
        "            iterations=100,\n",
        "            passes=20,\n",
        "            eval_every=None,\n",
        "            random_state=42,\n",
        "        )\n",
        "\n",
        "    coherence_uci = CoherenceModel(\n",
        "        model=lda_k,\n",
        "        texts=stopped_tokenized,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_uci'\n",
        "    )\n",
        "    uci = coherence_uci.get_coherence()\n",
        "\n",
        "    coherence_umass = CoherenceModel(\n",
        "        model=lda_k,\n",
        "        corpus=corpus,\n",
        "        dictionary=dictionary,\n",
        "        coherence='u_mass'\n",
        "    )\n",
        "    umass = coherence_umass.get_coherence()\n",
        "\n",
        "    coherence_cv = CoherenceModel(\n",
        "        model=lda_k,\n",
        "        texts=stopped_tokenized,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v'\n",
        "    )\n",
        "    c_v = coherence_cv.get_coherence()\n",
        "\n",
        "    history.append({'K': K, 'model': lda_k, 'c_v': c_v, 'umass': umass, 'uci': uci})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCrAu0r53vEb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(history).set_index('K')\n",
        "_ = df[['uci', 'umass', 'c_v']].plot.line(marker='.', figsize=(12, 12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgNyanKlEhWw"
      },
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"GridSearch Coherence\",\n",
        "          fontsize=20)\n",
        "\n",
        "plt.xlabel(\"K\", fontsize=14)\n",
        "plt.ylabel(\"Coherence\", fontsize=14)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "# Get the regular numpy array from the MaskedArray\n",
        "X_axis = df.index\n",
        "\n",
        "scoring = {'UMass': 'umass', 'UCI': 'uci', 'C_V': 'c_v'}\n",
        "\n",
        "for scorer, color in zip(sorted(scoring), ['tab:blue', 'tab:orange', 'tab:green']):\n",
        "  sample, style = ('test', '-')\n",
        "  scores = df[scoring[scorer]]\n",
        "  ax.plot(X_axis, scores, linestyle=style, color=color, label=scorer, marker='.')\n",
        "\n",
        "  best_index = scores.idxmax()\n",
        "  best_score = scores[best_index]\n",
        "\n",
        "  # Plot a dotted vertical line at the best score for that scorer marked by x\n",
        "  ax.plot([best_index, ] * 2, [0, best_score],\n",
        "          linestyle=':', color=color, marker='x', markeredgewidth=3, ms=8, alpha=0.4)\n",
        "\n",
        "  # Annotate the best score for that scorer\n",
        "  ax.annotate(f\"{best_index:d}\",\n",
        "              (best_index, 0.1), color=color, fontsize=14)\n",
        "\n",
        "  ax.annotate(f\"{best_score:0.2f}\",\n",
        "              (best_index+0.5, best_score + 0.1), color=color, fontsize=14)\n",
        "\n",
        "plt.legend(loc=\"best\", fontsize=16)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BV8XPtTBwbp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}