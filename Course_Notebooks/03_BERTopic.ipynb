{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP47Gl95Y8id"
      },
      "source": [
        "# Lecture 3 - Topic Modeling with BERTopic\n",
        "* Text Retrieval and Mining, BSc BAN, 2023-2024\n",
        "* Author: [Julien Rossi](mailto:j.rossi@uva.nl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Requisites\n",
        "\n",
        "* For this demo you need to have a API Key for OpenAI\n",
        "* It should be stored as a Notebook Secret under the name \"OPENAI_KEY\"\n",
        "* If no key is given, the demo will not use ChatGPT\n",
        "* ChatGPT is used to create a label to the topics"
      ],
      "metadata": {
        "id": "0CqX5H8pF-LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "* Official BERTopic [webpage](https://maartengr.github.io/BERTopic/index.html)"
      ],
      "metadata": {
        "id": "SdNi9_ElGxX8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83k6Y9z5iGuV"
      },
      "source": [
        "# Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7yUCfMuiIyo"
      },
      "source": [
        "We will use a dataset of BBC articles, see the [BBC Page](http://mlg.ucd.ie/datasets/bbc.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic openai typing-extensions==\"4.5.0\""
      ],
      "metadata": {
        "id": "nUZZ8LqWP8DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQt1eP3rpmOi"
      },
      "source": [
        "## Prepare Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjCcqp7MY2Ot"
      },
      "source": [
        "import requests\n",
        "\n",
        "r = requests.get('http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip')\n",
        "\n",
        "assert r.status_code == 200\n",
        "\n",
        "with open('bbc-fulltext.zip', 'wb') as out:\n",
        "    out.write(r.content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nwl5zJliY62"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "texts = []\n",
        "with ZipFile('bbc-fulltext.zip') as zf:\n",
        "    txtfiles = filter(lambda x: x.endswith('.txt'), zf.namelist())\n",
        "    for txtf in txtfiles:\n",
        "        with zf.open(txtf, 'r') as txt:\n",
        "            texts.append(txt.read().decode('utf-8', 'ignore').replace('\\n', ' '))\n",
        "\n",
        "print(f'Collected {len(texts)} articles.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kqprxgFlPGR"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling"
      ],
      "metadata": {
        "id": "XUJokqTYRkle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will prepare all the elements of the topic modeling pipeline\n",
        "* **EMBEDDING MODEL** transforms a document into a vector (more about this in Week 3). These vectors are called \"embeddings\"\n",
        "* **DIMENSIONALITY REDUCTION** model, to reduce the number of dimensions of the embeddings\n",
        "* **CLUSTERING ALGORITHM** that creates \"semantic\" cluster of documents on similar topics\n",
        "* **VECTORIZER** which will be used to extract the vocabulary of the corpus, and of the clusters\n",
        "* **REPRESENTATION MODEL** which will create labels from the top words in each topic"
      ],
      "metadata": {
        "id": "FK44TdaAHTTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Sentence Transformers to create document embeddings\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "HX_BfmKfRUmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction: UMAP\n",
        "from umap import UMAP\n",
        "umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine', random_state=42)\n",
        "\n",
        "# Clustering: HDBSCAN\n",
        "from hdbscan import HDBSCAN\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "# Vectorizer: CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
        "\n",
        "# Representation Model\n",
        "# ChatGPT if OPENAI API KEY is given\n",
        "# Otherwise Text-2-Text Generation Pipeline from Huggingface Transformers\n",
        "representation_model = None\n",
        "\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    openai_key = userdata.get('OPENAI_KEY')\n",
        "except Exception:\n",
        "    openai_key = None\n",
        "\n",
        "if openai_key is not None:\n",
        "    # Use ChatGpT\n",
        "    import openai\n",
        "    from bertopic.representation import OpenAI\n",
        "\n",
        "    # GPT-3.5\n",
        "    prompt = \"\"\"\n",
        "    I have a topic that contains the following documents:\n",
        "    [DOCUMENTS]\n",
        "    The topic is described by the following keywords: [KEYWORDS]\n",
        "\n",
        "    Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
        "    topic: <topic label>\n",
        "    \"\"\"\n",
        "    client = openai.OpenAI(api_key=openai_key)\n",
        "    representation_model = {\"ChatGPT\": OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt, doc_length=100, tokenizer=\"whitespace\")}\n",
        "else:\n",
        "    # Use a Text-2-Text Model from Transformers\n",
        "    from transformers import pipeline\n",
        "    from bertopic.representation import TextGeneration\n",
        "\n",
        "    prompt = \"I have a topic described by the following keywords: [KEYWORDS]. [DOCUMENTS] Describe this topic in less than 4 words, topic:\"\n",
        "\n",
        "    # Create your representation model\n",
        "    generator = pipeline('text2text-generation', model='google/flan-t5-base')\n",
        "    representation_model = {\"Flan T5\": TextGeneration(generator, prompt=prompt, nr_docs=1, doc_length=0, tokenizer=\"whitespace\")}\n"
      ],
      "metadata": {
        "id": "Nb3Tjie-c3EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    # Pipeline models\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    representation_model=representation_model,\n",
        "\n",
        "    # Hyperparameters\n",
        "    top_n_words=10,\n",
        "\n",
        "    # Running parameters\n",
        "    verbose=True,\n",
        "    calculate_probabilities=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "b5UZ65ngQfbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = topic_model.fit_transform(documents=texts, embeddings=embeddings)"
      ],
      "metadata": {
        "id": "kcPK-E5dXtzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "ef-JMT9gSUQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics in Document at index 0\n",
        "\n",
        "topic_model.visualize_distribution(probs[0])\n"
      ],
      "metadata": {
        "id": "6lvBQ3dXhaNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics Similarity\n",
        "\n",
        "topic_model.visualize_heatmap()\n"
      ],
      "metadata": {
        "id": "jffKX-56hi93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MO6jKoD5wnoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
