{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR0hChVWcOXZ"
      },
      "source": [
        "# BERT - Fine-Tuning for Movie Reviews\n",
        "* Text Retrieval and Mining, BSc BAN, 2023-2024\n",
        "* Author: [Julien Rossi](mailto:j.rossi@uva.nl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU Warning**\n",
        "\n",
        "This notebook requires a GPU to run properly."
      ],
      "metadata": {
        "id": "twV5k0YzrFO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates how we can use a model of the BERT family for a text classification task, using the huggingface library:\n",
        "* Load the data into a `Dataset`-like class: the class `IMDbDataset`\n",
        "  * Implements the operator `len` (method `__len__`) (remember Intro to Programming and dunder methods ?)\n",
        "  * Implements the operator `[]` (method `__getitem__`)\n",
        "  * Inherits from `torch.Dataset`\n",
        "  * The text is tokenized with a specific `Tokenizer`, loaded from a repository\n",
        "  * This tokenizer creates all the inputs for the BERT model\n",
        "* Set up the training arguments\n",
        "  * Parameters for the Deep Learning part of training\n",
        "  * Object of class `TrainingArguments`\n",
        "* Create a model ready for text classification\n",
        "  * loads pre-trained weights from a repository (using the name `'distilbert-base-uncased'`)\n",
        "  * The object of class `DistilBertForSequenceClassification` contains all the calculation graph from inputs to outputs\n",
        "* Fine-Tune the model on a text classification task\n",
        "  * Use the labeled data we have: text + label for training\n",
        "\n",
        "___\n",
        "\n",
        "This notebook shows how *easy* it is to summon a model with a few hundreds of millions of parameters and then fine-tune it on our specific classification task, thanks to [Huggingface](https://huggingface.co/docs/transformers/en/index).\n"
      ],
      "metadata": {
        "id": "g17EIRiToooW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "1njBMVnyrOoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBwTLpd1kNG3"
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "r = requests.get(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "assert r.status_code == 200"
      ],
      "metadata": {
        "id": "MarZrPN2sk-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import tarfile\n",
        "\n",
        "with tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\") as tgz:\n",
        "    tgz.extractall()"
      ],
      "metadata": {
        "id": "hSuC-1vustUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUpUrJGPd3kE"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEnJD9JieKHW"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, DistilBertConfig\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DhvrNLIeXyp"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            labels.append(0 if label_dir == \"neg\" else 1)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbSNtBm5eysJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cztXP2jHfE5m"
      },
      "source": [
        "MODEL = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onkf19dVgq2a"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9jKS-rgxOv"
      },
      "source": [
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrbSq7Mkyf-2"
      },
      "source": [
        "!rm -rf ./results\n",
        "!rm -rf ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKqfk-xyyRAv"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZeTrRFFj-qq"
      },
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    fp16=True,\n",
        "    do_train=True,\n",
        "    do_eval=True\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GbKApD8qqv4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
